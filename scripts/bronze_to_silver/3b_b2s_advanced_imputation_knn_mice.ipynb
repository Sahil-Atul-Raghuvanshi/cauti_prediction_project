{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Imputation: K-NN and MICE\n",
    "\n",
    "This notebook applies K-NN and MICE imputation to specific columns that benefit from relationship-aware imputation.\n",
    "\n",
    "**Target Columns:**\n",
    "- **MICE**: BP_systolic, BP_diastolic (strong correlation between pair)\n",
    "- **K-NN**: temperature, heart_rate, resp_rate, o2sat (vital signs cluster)\n",
    "- **K-NN**: creatinine, blood_wbc (multi-factor relationships)\n",
    "\n",
    "**Note**: This should be run AFTER temporal filling steps in `3_b2s_handle_missing_values.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.12.7)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get project root\n",
    "PROJECT_ROOT = Path(__file__).parent.parent.parent if '__file__' in globals() else Path.cwd().parent.parent\n",
    "\n",
    "# Load data after temporal filling (from previous notebook)\n",
    "# If running standalone, load the file saved after temporal filling\n",
    "file_path = PROJECT_ROOT / \"data\" / \"silver\" / \"bronze_outliers_handled.csv\"\n",
    "print(f\"Loading data from: {file_path}\")\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Apply Temporal Filling (if not already done)\n",
    "\n",
    "This replicates the temporal filling logic from the main notebook to ensure we have the baseline before applying K-NN/MICE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure datetime columns are properly formatted\n",
    "df[\"admittime\"] = pd.to_datetime(df[\"admittime\"], errors=\"coerce\")\n",
    "\n",
    "# Sort by class, patient, and time for temporal operations\n",
    "df = df.sort_values([\"y\", \"subject_id\", \"admittime\"])\n",
    "\n",
    "# Columns that benefit from temporal filling\n",
    "temporal_columns = [\n",
    "    \"BMI\", \"creatinine\", \"temperature\", \"heart_rate\", \n",
    "    \"resp_rate\", \"o2sat\", \"BP_systolic\", \"BP_diastolic\"\n",
    "]\n",
    "\n",
    "print(\"Applying temporal filling...\")\n",
    "for col in temporal_columns:\n",
    "    if col in df.columns:\n",
    "        # Forward fill within patient groups\n",
    "        df[col] = df.groupby([\"y\", \"subject_id\"])[col].ffill()\n",
    "        # Backward fill within patient groups\n",
    "        df[col] = df.groupby([\"y\", \"subject_id\"])[col].bfill()\n",
    "\n",
    "print(\"Temporal filling completed.\")\n",
    "\n",
    "# Check remaining missing values\n",
    "missing_after_temporal = df[temporal_columns].isna().sum()\n",
    "print(\"\\nMissing values after temporal fill:\")\n",
    "print(missing_after_temporal[missing_after_temporal > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: MICE Imputation for Blood Pressure Pair\n",
    "\n",
    "BP_systolic and BP_diastolic have a strong correlation. MICE can model this relationship better than simple group medians.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mice_bp(df):\n",
    "    \"\"\"Apply MICE imputation to BP_systolic and BP_diastolic.\"\"\"\n",
    "    \n",
    "    bp_columns = ['BP_systolic', 'BP_diastolic']\n",
    "    \n",
    "    # Check if columns exist and have missing values\n",
    "    if not all(col in df.columns for col in bp_columns):\n",
    "        print(\"BP columns not found. Skipping MICE for BP.\")\n",
    "        return df\n",
    "    \n",
    "    missing_bp = df[bp_columns].isna().any(axis=1).sum()\n",
    "    if missing_bp == 0:\n",
    "        print(\"No missing BP values. Skipping MICE.\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"Rows with missing BP: {missing_bp} ({missing_bp/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Features to use for BP imputation\n",
    "    bp_features = [\n",
    "        'BP_systolic', 'BP_diastolic', 'heart_rate', 'anchor_age', \n",
    "        'BMI', 'temperature', 'icu_admission'\n",
    "    ]\n",
    "    \n",
    "    # Filter to available features\n",
    "    bp_features = [f for f in bp_features if f in df.columns]\n",
    "    \n",
    "    # Prepare data\n",
    "    bp_data = df[bp_features].copy()\n",
    "    \n",
    "    # Handle categorical variables\n",
    "    if 'icu_admission' in bp_data.columns:\n",
    "        bp_data = pd.get_dummies(bp_data, columns=['icu_admission'], drop_first=True, prefix='icu')\n",
    "    \n",
    "    # Store original indices\n",
    "    original_indices = bp_data.index\n",
    "    \n",
    "    # Apply MICE\n",
    "    print(\"Applying MICE imputation for BP...\")\n",
    "    \n",
    "    # Use RandomForest as estimator for better handling of non-linear relationships\n",
    "    mice_imputer = IterativeImputer(\n",
    "        estimator=RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1),\n",
    "        max_iter=10,\n",
    "        random_state=42,\n",
    "        imputation_order='ascending',  # Start with least missing\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    bp_data_imputed = mice_imputer.fit_transform(bp_data)\n",
    "    bp_data_imputed = pd.DataFrame(bp_data_imputed, columns=bp_data.columns, index=original_indices)\n",
    "    \n",
    "    # Update only the BP columns in original dataframe\n",
    "    df.loc[df.index, 'BP_systolic'] = bp_data_imputed['BP_systolic']\n",
    "    df.loc[df.index, 'BP_diastolic'] = bp_data_imputed['BP_diastolic']\n",
    "    \n",
    "    print(f\"MICE imputation for BP completed.\")\n",
    "    print(f\"Remaining missing BP_systolic: {df['BP_systolic'].isna().sum()}\")\n",
    "    print(f\"Remaining missing BP_diastolic: {df['BP_diastolic'].isna().sum()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply MICE for BP\n",
    "df = apply_mice_bp(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: K-NN Imputation for Vital Signs Cluster\n",
    "\n",
    "Temperature, heart_rate, resp_rate, and o2sat are highly correlated. K-NN can leverage these relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_knn_vitals(df, n_neighbors=5):\n",
    "    \"\"\"Apply K-NN imputation to vital signs cluster.\"\"\"\n",
    "    \n",
    "    vital_signs = ['temperature', 'heart_rate', 'resp_rate', 'o2sat']\n",
    "    \n",
    "    # Check if columns exist\n",
    "    vital_signs = [v for v in vital_signs if v in df.columns]\n",
    "    if len(vital_signs) == 0:\n",
    "        print(\"Vital signs columns not found. Skipping K-NN.\")\n",
    "        return df\n",
    "    \n",
    "    missing_vitals = df[vital_signs].isna().any(axis=1).sum()\n",
    "    if missing_vitals == 0:\n",
    "        print(\"No missing vital signs. Skipping K-NN.\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"Rows with missing vital signs: {missing_vitals} ({missing_vitals/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Features to use for vital signs imputation\n",
    "    vital_features = vital_signs + [\n",
    "        'anchor_age', 'BMI', 'icu_admission', 'BP_systolic', 'BP_diastolic'\n",
    "    ]\n",
    "    \n",
    "    # Filter to available features\n",
    "    vital_features = [f for f in vital_features if f in df.columns]\n",
    "    \n",
    "    # Prepare data\n",
    "    vital_data = df[vital_features].copy()\n",
    "    \n",
    "    # Handle categorical variables\n",
    "    if 'icu_admission' in vital_data.columns:\n",
    "        vital_data = pd.get_dummies(vital_data, columns=['icu_admission'], drop_first=True, prefix='icu')\n",
    "    \n",
    "    # Store original indices and column names\n",
    "    original_indices = vital_data.index\n",
    "    vital_cols = vital_data.columns.tolist()\n",
    "    \n",
    "    # Scale features for K-NN (important for distance calculation)\n",
    "    print(\"Scaling features for K-NN...\")\n",
    "    scaler = StandardScaler()\n",
    "    vital_data_scaled = scaler.fit_transform(vital_data)\n",
    "    \n",
    "    # Apply K-NN imputation\n",
    "    print(f\"Applying K-NN imputation (k={n_neighbors}) for vital signs...\")\n",
    "    knn_imputer = KNNImputer(\n",
    "        n_neighbors=n_neighbors,\n",
    "        weights='distance',  # Weight by distance\n",
    "        metric='euclidean'\n",
    "    )\n",
    "    \n",
    "    vital_data_imputed = knn_imputer.fit_transform(vital_data_scaled)\n",
    "    \n",
    "    # Inverse transform to get original scale\n",
    "    vital_data_imputed = scaler.inverse_transform(vital_data_imputed)\n",
    "    vital_data_imputed = pd.DataFrame(vital_data_imputed, columns=vital_cols, index=original_indices)\n",
    "    \n",
    "    # Update only the vital signs columns in original dataframe\n",
    "    for col in vital_signs:\n",
    "        if col in vital_data_imputed.columns:\n",
    "            missing_mask = df[col].isna()\n",
    "            df.loc[missing_mask, col] = vital_data_imputed.loc[missing_mask, col]\n",
    "    \n",
    "    print(f\"K-NN imputation for vital signs completed.\")\n",
    "    for col in vital_signs:\n",
    "        remaining = df[col].isna().sum()\n",
    "        print(f\"  Remaining missing {col}: {remaining}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply K-NN for vital signs\n",
    "df = apply_knn_vitals(df, n_neighbors=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: K-NN Imputation for Lab Values (Creatinine and Blood WBC)\n",
    "\n",
    "These have complex multi-factor relationships that K-NN can capture better than group medians.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_knn_labs(df, n_neighbors=5):\n",
    "    \"\"\"Apply K-NN imputation to creatinine and blood_wbc.\"\"\"\n",
    "    \n",
    "    lab_columns = ['creatinine', 'blood_wbc']\n",
    "    \n",
    "    # Check if columns exist\n",
    "    lab_columns = [l for l in lab_columns if l in df.columns]\n",
    "    if len(lab_columns) == 0:\n",
    "        print(\"Lab columns not found. Skipping K-NN.\")\n",
    "        return df\n",
    "    \n",
    "    missing_labs = df[lab_columns].isna().any(axis=1).sum()\n",
    "    if missing_labs == 0:\n",
    "        print(\"No missing lab values. Skipping K-NN.\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"Rows with missing lab values: {missing_labs} ({missing_labs/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Process each lab column separately (they may have different relationships)\n",
    "    for lab_col in lab_columns:\n",
    "        if df[lab_col].isna().sum() == 0:\n",
    "            print(f\"No missing values for {lab_col}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing {lab_col}...\")\n",
    "        \n",
    "        # Features to use for lab imputation\n",
    "        if lab_col == 'creatinine':\n",
    "            lab_features = [lab_col, 'anchor_age', 'BMI', 'blood_wbc', \n",
    "                          'BP_systolic', 'BP_diastolic', 'gender', 'temperature']\n",
    "        elif lab_col == 'blood_wbc':\n",
    "            lab_features = [lab_col, 'temperature', 'heart_rate', 'creatinine', \n",
    "                          'anchor_age', 'BMI', 'gender']\n",
    "        else:\n",
    "            lab_features = [lab_col, 'anchor_age', 'BMI', 'gender']\n",
    "        \n",
    "        # Filter to available features\n",
    "        lab_features = [f for f in lab_features if f in df.columns]\n",
    "        \n",
    "        # Prepare data\n",
    "        lab_data = df[lab_features].copy()\n",
    "        \n",
    "        # Handle categorical variables\n",
    "        if 'gender' in lab_data.columns:\n",
    "            lab_data = pd.get_dummies(lab_data, columns=['gender'], drop_first=True, prefix='gender')\n",
    "        \n",
    "        # Store original indices and column names\n",
    "        original_indices = lab_data.index\n",
    "        lab_cols = lab_data.columns.tolist()\n",
    "        \n",
    "        # Scale features for K-NN\n",
    "        scaler = StandardScaler()\n",
    "        lab_data_scaled = scaler.fit_transform(lab_data)\n",
    "        \n",
    "        # Apply K-NN imputation\n",
    "        print(f\"  Applying K-NN imputation (k={n_neighbors})...\")\n",
    "        knn_imputer = KNNImputer(\n",
    "            n_neighbors=n_neighbors,\n",
    "            weights='distance',\n",
    "            metric='euclidean'\n",
    "        )\n",
    "        \n",
    "        lab_data_imputed = knn_imputer.fit_transform(lab_data_scaled)\n",
    "        \n",
    "        # Inverse transform\n",
    "        lab_data_imputed = scaler.inverse_transform(lab_data_imputed)\n",
    "        lab_data_imputed = pd.DataFrame(lab_data_imputed, columns=lab_cols, index=original_indices)\n",
    "        \n",
    "        # Update only the lab column in original dataframe\n",
    "        missing_mask = df[lab_col].isna()\n",
    "        df.loc[missing_mask, lab_col] = lab_data_imputed.loc[missing_mask, lab_col]\n",
    "        \n",
    "        remaining = df[lab_col].isna().sum()\n",
    "        print(f\"  Remaining missing {lab_col}: {remaining}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply K-NN for lab values\n",
    "df = apply_knn_labs(df, n_neighbors=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Fallback to Group Median (for any remaining missing values)\n",
    "\n",
    "If K-NN/MICE couldn't impute some values (e.g., all neighbors also missing), fall back to group medians.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fallback_median(df):\n",
    "    \"\"\"Apply group median as fallback for any remaining missing values.\"\"\"\n",
    "    \n",
    "    # Columns that might still have missing values\n",
    "    target_columns = [\n",
    "        'BP_systolic', 'BP_diastolic', 'temperature', 'heart_rate', \n",
    "        'resp_rate', 'o2sat', 'creatinine', 'blood_wbc', 'BMI'\n",
    "    ]\n",
    "    \n",
    "    target_columns = [c for c in target_columns if c in df.columns]\n",
    "    \n",
    "    # Create age_group if not exists\n",
    "    if 'age_group' not in df.columns:\n",
    "        df['age_group'] = pd.cut(\n",
    "            df['anchor_age'],\n",
    "            bins=[0, 18, 40, 65, 120],\n",
    "            labels=['child', 'adult', 'middle_age', 'elderly']\n",
    "        )\n",
    "    \n",
    "    print(\"Applying fallback group medians...\")\n",
    "    \n",
    "    for col in target_columns:\n",
    "        if df[col].isna().sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        # Group median by (y, age_group, gender) or (y, age_group, icu_admission)\n",
    "        if col in ['o2sat', 'BP_systolic', 'BP_diastolic']:\n",
    "            group_key = ['y', 'age_group', 'icu_admission']\n",
    "        else:\n",
    "            group_key = ['y', 'age_group', 'gender']\n",
    "        \n",
    "        # Compute group median\n",
    "        group_median = df.groupby(group_key, observed=False)[col].median()\n",
    "        \n",
    "        # Join back\n",
    "        df = df.join(\n",
    "            group_median.rename(f'{col}_group_median'),\n",
    "            on=group_key\n",
    "        )\n",
    "        \n",
    "        # Fallback to overall median by y\n",
    "        overall_median_by_y = df.groupby('y')[col].median()\n",
    "        \n",
    "        df[f'{col}_group_median'] = df.apply(\n",
    "            lambda row: (\n",
    "                overall_median_by_y.loc[row['y']]\n",
    "                if pd.isna(row[f'{col}_group_median'])\n",
    "                else row[f'{col}_group_median']\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Fill missing\n",
    "        df[col] = df[col].fillna(df[f'{col}_group_median'])\n",
    "        \n",
    "        # Drop helper column\n",
    "        df = df.drop(columns=[f'{col}_group_median'])\n",
    "        \n",
    "        remaining = df[col].isna().sum()\n",
    "        if remaining > 0:\n",
    "            print(f\"  {col}: {remaining} missing values remain (using overall median)\")\n",
    "            overall_median = df[col].median()\n",
    "            df[col] = df[col].fillna(overall_median)\n",
    "    \n",
    "    # Cleanup age_group if it was created here\n",
    "    df = df.drop(columns=['age_group'], errors='ignore')\n",
    "    \n",
    "    print(\"Fallback medians applied.\")\n",
    "    return df\n",
    "\n",
    "# Apply fallback\n",
    "df = apply_fallback_median(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Validation and Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check final missing values\n",
    "target_columns = [\n",
    "    'BP_systolic', 'BP_diastolic', 'temperature', 'heart_rate', \n",
    "    'resp_rate', 'o2sat', 'creatinine', 'blood_wbc', 'BMI'\n",
    "]\n",
    "\n",
    "target_columns = [c for c in target_columns if c in df.columns]\n",
    "\n",
    "print(\"Final Missing Values Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'missing_count': df[target_columns].isna().sum(),\n",
    "    'missing_percentage': (df[target_columns].isna().mean() * 100).round(2)\n",
    "}).sort_values('missing_percentage', ascending=False)\n",
    "\n",
    "print(missing_summary)\n",
    "\n",
    "total_missing = missing_summary['missing_count'].sum()\n",
    "print(f\"\\nTotal missing values: {total_missing}\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "\n",
    "if total_missing == 0:\n",
    "    print(\"\\n✓ All target columns successfully imputed!\")\n",
    "else:\n",
    "    print(f\"\\n⚠ {total_missing} missing values remain.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the imputed dataset\n",
    "output_path = PROJECT_ROOT / \"data\" / \"silver\" / \"bronze_missing_values_handled_advanced.csv\"\n",
    "\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Dataset saved to: {output_path}\")\n",
    "print(f\"Final shape: {df.shape}\")\n",
    "\n",
    "# Optionally, also save comparison summary\n",
    "comparison_path = PROJECT_ROOT / \"data\" / \"silver\" / \"imputation_comparison_summary.csv\"\n",
    "missing_summary.to_csv(comparison_path, index=True)\n",
    "print(f\"Comparison summary saved to: {comparison_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "1. **Performance**: K-NN and MICE are computationally expensive. For 158K rows, this may take 10-30 minutes.\n",
    "\n",
    "2. **Memory**: Ensure sufficient RAM (recommended: 8GB+).\n",
    "\n",
    "3. **Comparison**: Compare results with the baseline approach to evaluate improvement.\n",
    "\n",
    "4. **Tuning**: Adjust `n_neighbors` for K-NN (default: 5) based on your data characteristics.\n",
    "\n",
    "5. **Integration**: This notebook can be integrated into the main pipeline or run separately for comparison.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
